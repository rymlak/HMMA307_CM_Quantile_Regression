\documentclass[unknownkeysallowed]{beamer}
\usepackage[french,english]{babel}
\usepackage{beamer_js}
\usepackage{shortcuts_js}
\usepackage{etex}
\usepackage{csquotes}
\usepackage{fourier}
\nocite{*}
\addbibresource{biblio.bib}


\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%             Headers               %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\bigskip
\bigskip
\begin{center}{
\LARGE\color{marron}
\textbf{HMMA307 : \\ Advanced Linear Modeling}
\textbf{ }\\
\vspace{0.5cm}
}

\color{marron}
\textbf{Chapter 5 : Random Anova}
\end{center}
\vspace{0.5cm}

\begin{center}
\textbf{Part I: Fanchon Herman \ Cassandre Lepercque \ Cherif Amghar} \\
\textbf{Part II: Yani Bouaffad \ Ryma Lakehal \ Loïc Sauton}\\
\vspace{0.1cm}
\url{https://github.com/ybouaffad/HMMA307_CM_Quantile_Regression}\\
\vspace{0.3cm}
Université de Montpellier \\
\end{center}

\centering
\includegraphics[width=0.13\textwidth]{Logo}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%       PLAN      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Table of Contents}
\tableofcontents[hideallsubsections]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
\label{sec:motiv}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Motivation}
Mixed models can be used in practice to deal with disordered data and allow us to use all of our data. Indeed, we can have different grouping factors but also small sample sizes. So, mixed models can process the data even when we have small sample sizes, structured data, and many covariates to fit.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statistical model}
\label{sec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Statistical model}
\mytheorem{Model equation}
{\[y_{ij}=\mu^*+ A_j+\varepsilon_{ij}\]}

\medskip

 \begin{itemize}
        \item $\mu^* \in \bbR$, fixed effect,
        \item $A_j \overset{\iid}{\sim} \mathcal{N}(0, \sigma_A^2), \text{ } \sigma_A^2>0, \text{ } \forall j \in \llbracket 1,J \rrbracket$, random effect,
        \item $\varepsilon_{ij} \overset{\iid}{\sim} \mathcal{N}(0,\sigma_{\varepsilon}^2) \text{ } \sigma_{\varepsilon}^2>0, \text{ } \forall i \in \llbracket 1,I \rrbracket \text{ ,} \forall j \in \llbracket 1,J \rrbracket$ the noise,
        \item $A_j \perp \!\!\! \perp \varepsilon_{ij} \text{ ,}\forall i,j$,
        \item $n=\sum_{j=1}^{J} n_j$ and $n_j$ the number of observations of the modality $J$.
    \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Esperance and covariance}
\begin{itemize}
    \item $\bbE[y_{ij}]=\mu^* $, 
    \item $\bbV(y_{ij})=\sigma_A^2 + \sigma_{\varepsilon}^2$,
    \item $\cov(y_{ij},y_{i'j'})=\sigma_A^2\delta_{jj'} + \sigma_{\varepsilon}^2\delta_{ii'}\delta_{jj'}$,
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix model}
\label{sub:matrix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% MATRIX MODEL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Matrix Model}
\mytheorem{Model equation}
{\[y=\mu^* \bbI_n+ ZA+\varepsilon\]}
\medskip

 \begin{itemize} 
        \item $\mu^* \in \bbR$, fixed effect
        \item $Z = \begin{bmatrix} \bbI_{C_1} \  \cdots \  \bbI_{C_J} \end{bmatrix} \in \bbR^{n \times J}$, design matrix, 
        \item $C_1\sqcup \ \cdots \ \sqcup C_J = \llbracket 1,n \rrbracket$, classes / modalities,
        \item $A = \begin{pmatrix}
                    A_1 \ \cdots \ A_J
                    \end{pmatrix}^T \in \bbR^J, \ 
        A \sim \cN(0, \sigma_A^2 \ I_{d_J}), \text{ } \sigma_A^2>0$, random matrix,
        \item $\varepsilon \sim \cN(0,\sigma_{\varepsilon}^2) \text{ , } \sigma_{\varepsilon}^2>0,$ the noise,
        \item \[ ZA = \sum_{j=1}^J A_j \bbI_{C_j} \in \bbR^n. \]
    \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% VARIANCE CALCULATE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Variance calculate}

    \begin{itemize} 
        \item We have that, $\bbV(ZA) = \underbrace{Z}_{n\times J} \underbrace{\bbV(A)}_{J\times J} \  \underbrace{Z^T}_{J\times n} = \sigma^2_A Z Z^T \ \in \bbR^{n\times n}$, 
        \item Where, \[ ZZ^T = \begin{bmatrix} \bbI_{C_1}\  \cdots \ \bbI_{C_J} \end{bmatrix} \begin{bmatrix}
        \bbI_{C_1}^T\\
        \vdots \\
        \bbI_{C_J}^T
        \end{bmatrix} = \sum_{j=1}^J \bbI_{C_j} \bbI_{C_j}^T, \]
        \item Then, $\bbV(y) = \sigma^2_A ZZ^T + \sigma^2_{\varepsilon} I_{d_n}$.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% REMINDER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Reminder for the statistical model}

    \begin{itemize} 
        \item The model : $y_{ij} = \mu^* + A_j + \varepsilon_{ij}$, 
        \item We have that \[ \overline{y_{:j}} = \frac{1}{n_j}\sum_{i \in C_j} y_{ij}, \] 
        \item So, $\bbV(\overline{y_{:j}}) = \sigma_
        A^2 + \frac{\sigma_{\varepsilon}^2}{n_j} := \tau_j^2$ \ and \  $\bbE(\overline{y_{:j}}) = \mu^* $ without bias.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% MU ESTIMATOR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{$\mu^*$-estimator}
\mytheorem{Formula}
{\[ \hat{\mu} = \sum_{j=1}^J \omega_j \overline{y_{:j}} , \hspace{0.3cm}  \text{with} \hspace{0.2cm} \omega_j \propto \frac{1}{\bbV(\overline{y_{:j}})} \text{the weighting}. \]}
\medskip

\vspace{1cm}
\underline{\textbf{Remark :}}\\
In balance case, we have :
\begin{itemize}
    \item $n_j = I$, as we already know that $n = IJ$,
    \item So we obtain, \[ \hat{\mu}=\frac{I}{J}\sum_{j=1}^J\overline{y_{:j}}. \]
\end{itemize}
     
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%   THEOREM    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\visible<1->{
\begin{alertblock}{Theorem}
Let $X_1$,...,$X_n$ independent random variables. We suppose that $E(X_1)=....=E(X_n)= \mu^{*} $
Among the linear unbiased estimators in $X_1$, ....,$X_n$ of $\mu^{*}$, the minimal variance estimator is denoted by $\hat{\mu}$ and is worth:
$$\hat{\mu}=\sum_{i=1}^{n}\frac{X_{i}/var(X_{i})}{\sum_{i'=1}^{n}1/var(X_{i})}$$
\end{alertblock}
}

\begin{itemize}\setlength{\itemsep}{5pt}
\visible<1->{\item  \textbf{Demo}}:
We have $$ min\limits_{(\alpha_{1},...,\alpha_{n})\in \mathbb{R}^{n}} Var(\sum\limits_{i=1}^{n}\alpha_{i}X_{i}) $$
u.c: \sum\limits_{i=1}^{n}\alpha_{i}=1

$$E(\sum\limits_{i=1}^{n}\alpha_{i}X_{i}) = \mu^{*}$$
$$Var(\sum\limits_{i=1}^{n}\alpha_{i}X_{i})=\sum\limits_{i=1}^{n}\alpha_{i}^{2}Var(X_{i})$$

\end{itemize}
\end{frame}

\begin{frame}
Minimize the last expression amounts to minimize this expression: 
    $$ min\limits_{(\alpha_{1},...,\alpha_{n})\in \mathbb{R}^{n}} \sum\limits_{i=1}^{n}\alpha_{i}^{2}Var(X_{i}) \quad u.c \quad  \sum\limits_{i=1}^{n}\alpha_{i}=1 $$
\begin{itemize}\setlength{\itemsep}{5pt}
\visible<1->{\item \textbf{Lagrangian} : $$\mathcal{L}(\alpha,\lambda)=\sum\limits_{i=1}^{n}\alpha_{i}^{2}Var(X_{i}) + \lambda (\sum\limits_{i=1}^{n}\alpha_i -1) $$}
\end{itemize}
Resolution of the optimization system:
\[\nabla \mathcal{L}(\hat\alpha,\hat\lambda)=0\]
\[
\begin{aligned}
\begin{cases}
&  \frac{\partial \mathcal{L}}{\partial \alpha}=0 \\
&  \frac{\partial \mathcal{L}}{\partial \alpha_{i_0}}=0\ \forall i_0 
\end{cases}
\quad&\Longleftrightarrow\quad
\begin{cases}
&  \sum\limits_{i=1}^{n}\hat{\alpha}_i=1 \\
&  2\hat\alpha_{i_0}Var(x_{i_0} + \hat\lambda =0, \forall{i_0}  \\
\end{cases}\\
& \Longleftrightarrow\quad
\begin{cases}
&  \hat\alpha_{i_0}=\frac{-\hat\lambda}{2Var(x_{i_0}} \\
& \sum\limits_{i_{0}=1}^{n}\hat{\alpha}_i_{0}=1=\frac{-\hat\lambda}{2}(\sum_{i_{0}=1}^{n}\frac{1}{Var(x_{i_0})}) \\

\end{cases}
\end{aligned}
\]
\end{frame}
\begin{frame}
    Finally, $$ \hat \lambda = -2(\frac{1}{\sum_{i=1}^{n}1/var(X_{i})})$$
    
     $$\hat\alpha_{i_0}=\frac{\frac{1}{Var(X_i_0)}}{\sum_{i=1}^{n}1/var(X_{i})} 
    \Longrightarrow \hat\mu = \sum_{j=1}^{J}\alpha_{j}\bar{y_{ij}}$$
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% VARIANCE ESTIMATOR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Variance Estimator}
\begin{itemize}\setlength{\itemsep}{5pt}
\visible<1->{\item{$\sigma_\epsilon^2$}}: $$\mathbb{E} \left[ \frac{1}{J-1}\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{n_j}(y_{ij} - \overline{y_{:j}})^2\right] &= \sigma_\epsilon^2$$
a possible estimator unbiased for $\sigma_\epsilon^2$ is:
\\
$$\boxed{
\hat{\sigma}_\epsilon^2 &=   \frac{1}{J-1}\sum\limits_{j=1}^{J}\sum\limits_{i=1}^{n_j} (y_{ij} - \overline{y_{:j}})^2}$$

\end{itemize}
\end{frame}

\begin{frame}

\begin{itemize}\setlength{\itemsep}{5pt}
\visible<1->{\item{\sigma_A^2}:\n
\\
\n
We have:
\\
$$
\mathbb{E} \left[ \frac{1}{J-1}\sum\limits_{j=1}^{J}\sum\limits_{i=1}^{n_j}(\overline{y_{:j}} - \overline{y_n})^2\right] &= \sigma_\epsilon^2   + \frac{1}{n(J-1)}\left[n^2 - \sum\limits_{j=1}^J n_j^2\right]  \sigma_A^2
$$
\\
Therefore,
\\

$$ 
\boxed{
\hat{\sigma}_A^2 &= \frac{\hat{S^2}- \hat{\sigma_\epsilon}^2}{n^2 - \sum\limits_{j=1}^J n_j^2}
}
$$

\\
 with $\hat{S^2} = \frac{1}{J-1} \sum\limits_{j=1}^{J} n_j(\overline{y_{:j}} - \overline{y_n})^2$
}
\end{itemize}
\end{frame}

\begin{frame}
\underline{\textbf{Remarks :}}\\
\begin{itemize}
    \item  $n^2 >\sum \limits_{j=1}^J n_j$ ,
   	\item Nothing guarantees that $\hat{\sigma}_A^2 \geq 0$  and otherwise the model is unsuitable for data ( where  $\sigma_A^2 \approx 0$ ) . In general, it is replaced by 0 in this case. \\
 So $\hat{\sigma}_A^2  = max( 0, \hat{\sigma}_A^2)$
	\item $\hat{\mu} &= \sum \limits_{j=1}^J w_j \overline{y_{:j}}$  is not an estimator because it is not a function of  data, it depends on $\sigma_A^2$ and $\sigma_\epsilon^2$.\\   We define $\tilde{\mu}$ as: \\
$$\tilde{\mu} &= \sum\limits_{j=1}^J \tilde {w_j}\overline{y_{:j}} \text{ with } \tilde {w_j} \propto \left(\hat{\sigma}_A^2 + \frac{\hat{\sigma}_\epsilon^2}{n}\right)^{-1}$$
\end{itemsize} 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% MAXIMUM LIKELIHOOD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Another approach: Maximum Likelihood}
\underline{\textbf{Reminder of model :}}\\
\begin{itemize}
        \item Equation: $y=\mu^* \bbI_n+ ZA+\varepsilon$
        \item $Z = \begin{bmatrix} \bbI_{C_1} \  \cdots \  \bbI_{C_J} \end{bmatrix} \in \bbR^{n \times J}$
        \item $A = \begin{pmatrix}
                    A_1 \ \cdots \ A_J
                    \end{pmatrix}^T \in \bbR^J, \ 
        A \sim \cN(0, \sigma_A^2 \ I_{d_J}), \text{ } \sigma_A^2>0$, random matrix,
        \item $\varepsilon \sim \cN(0,\sigma_{\varepsilon}^2) \text{ , } \sigma_{\varepsilon}^2>0,$ the noise.
        \item And
\end{itemize}\\
\vspace{0.3cm}
$$\boxed{ y \sim\cN \left( \mu^* \bbI_n \ ,\sigma^2_A ZZ^T + \sigma^2_{\varepsilon} I_{d_n}\right) } $$
\end{frame}

\begin{frame}
\begin{itemize}\setlength{\itemsep}{5pt}
\visible<1->{\item \textbf{Likelihood} :
\\
$$\mathcal{L}(y,\mu,\hat\sigma_{A}^{2},\hat\sigma_{\epsilon}^{2}) \;  \propto \lvert V \rvert^{-1/2}\ exp(-\frac12 \left[y - \mu\bbI_{n}\right]^{T} \ V^{-1} \left[y - \mu\bbI_{n}\right])$$} \\

$$-log(\mathcal{L}) = \frac12(y - \mu\bbI_{n})^{T} V^{-1}(y - \mu\bbI_{n}) + \frac12 log \lvert V \rvert + \frac{n}{2}log(2\pi)$$\\
We want to minimize $-log(\mathcal{L})$

$$\Longrightarrow\quad \underset{\mu,\sigma_{A}^{2},\sigma_{\epsilon}^{2}}{min} \frac12 (y - \mu\bbI_{n})^{T} V^{-1} (y - \mu\bbI_{n}) + \frac12 log \lvert V \rvert$$
\end{itemize}
\end{frame}


\begin{frame}

\begin{itemize}\setlength{\itemsep}{5pt}
\visible<1->{\item \textbf{first condition order on}} $\hat\mu$ :

$$\dfrac{\partial log(\mathcal{L})}{\partial \hat\mu} = 0$$
\\
\Longleftrightarrow\quad $$\hat{\mu} = \dfrac{\bbI_{n}^{T}V^{-1}y}{\bbI_{n}^{T}V^{-1}\bbI_{n}}$$\\

\vspace{0.5cm}
This is the estimator $\hat\mu$ that we proposed before.\\
\vspace{0.5cm}
\underline{\textbf{Remark :}}\\
\vspace{0.2cm}
Optimization in $\sigma_{A}^{2}$ and $\sigma_{\epsilon}^{2}$ doesn't admit an explicit function.
\end{itemize}
\end{frame}

\begin{frame}{Alternate algorithm}
\vspace{0.2cm}
\warning \; V is unknown V = \sigma_{\epsilon}^{2} I_{d} +  \sigma_{A}^{2} Z Z^{T}

\vspace{0.2cm}
\begin{itemize}\setlength{\itemsep}{5pt}
\visible<1->{\item \textbf{Alternate}} :
\end{itemize}

\vspace{0.3cm}
    
\textbf{1.} estimate $\mu$ with V_{known} ($\sigma_{A}^{2}$,$\sigma_{\epsilon}^{2}$ known and fixed)\\
\vspace{0.5cm}
\textbf{2.} estimate $\sigma_{\epsilon}^{2}$ with \ $\widehat\mu$ and $\sigma_{A}^{2}$ known and fixed\\
\vspace{0.5cm}
\textbf{3.} estimate $\sigma_{\epsilon}^{2}$ with $\widehat\mu$ and $\sigma_{A}^{2}$ known and fixed\\
\vspace{0.5cm}
\underline{\textbf{Remarks :}}\\
\vspace{0.2cm}

Under the assumption of regularity of the function

$$(\mu,\sigma_{\epsilon}^{2},\sigma_{A}^{2}) \mapsto -log(\mathcal{L})$$

then the alternate minimization algorithm converges towards a critical point.
Finally if the function to be optimized is more convex, we obtain a global minimum.
\end{frame}





\end{document}