\documentclass[unknownkeysallowed]{beamer}
\usepackage[french,english]{babel}
\usepackage{beamer_js}
\usepackage{shortcuts_js}
\usepackage{etex}
\usepackage{csquotes}
\usepackage{fourier}
\nocite{*}
\addbibresource{biblio.bib}

% importer bibilo !!!!!!!!!!!!!! voir projet file attente
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%             Headers               %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\bigskip
\bigskip
\begin{center}{
\LARGE\color{marron}
\textbf{HMMA307 : \\ Advanced Linear Modeling}
\textbf{ }\\
\vspace{0.5cm}
}

\color{marron}
\textbf{Chapter 7 : Quantile Regression}
\end{center}

\vspace{0.5cm}

\begin{center}
\textbf{Yani Bouaffad \ Ryma Lakehal \ Loïc Sauton} \\
\vspace{0.1cm}
\url{https://github.com/ybouaffad/HMMA307_CM_Quantile_Regression}\\
\vspace{0.5cm}
Université de Montpellier \\
\end{center}

\centering
\includegraphics[width=0.13\textwidth]{Logo}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\AtBeginSection[]
{
\begin{frame}<beamer>{Table of Contents}
\tableofcontents[currentsubsection,
    hideothersubsections,
    sectionstyle=show/shaded,
]
\end{frame}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introdcution}

\begin{frame}{Introduction}
\vspace{0.2cm}
Classical linear regression estimates the mean response of the dependent variable dependent on the independent variables. There are many cases, such as skewed data, multimodal data, or data with outliers, when the behavior at the conditional mean fails to fully capture the patterns in the data.
\vspace{0.2cm}
\\
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reminder : Median/Quantiles}
\label{sec:Reminder : Median/Quantiles}
\subsection{Median}
\label{sub:Median}

\begin{frame}{Reminder : Median/Quantiles}

\vspace{0.4cm}

\mytheorem{Median Definition}
{
Let $y_1, \dots , y_n \in \mathbb{R} $, we have :
$$Med_{n}(y_1,\dots,y_n)\in \underset{ \mu \in \mathbb{R}} \argmin \quad \frac{1}{n} \sum_{i=1}^n |y_i-\mu|$$
}

\vspace{0.25cm}


\begin{figure}[H]
    \centering
    \includegraphics[scale=.45]{n=1.png}
    \caption{Optimization function for n=1}
    \label{fig}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\begin{figure}[H]
    \centering
    \includegraphics[scale=.40]{n=2 et n=3.png}
    \caption{Optimization function for n=2 and for n=3}
    \label{fig}
\end{figure}

for 2 observations $y_1; y_2$ all points between $y_1$ and $y_2$ are global minimums

for 3 observations $y_1; y_2; y_3$ the optimization function admits a unique global minimizer

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
 The optimization problem depends on the parity of the points :\\
 In practice, for  $y_{(1)} \leqslant \dots \leqslant y_{(n)}$ we have :
 
 $$Med_{n}(y_{1}, \dots, y_{n}) =
\left\{
    \begin{array}{ll}
        y_{\lfloor\frac{n}{2}\rfloor+1} \quad \text{if $n$ is odd}\\
        \frac{y_{\lfloor\frac{n}{2}\rfloor}+y_{\lfloor\frac{n}{2}\rfloor+1}}{2} \quad \text{if $n$ is even}
    \end{array}
\right.
$$
\vspace{1cm}
\underline{\textbf{Remark :}}\\
$$\begin{array}{l|rcl}
    f : & \mathbb{R} & \longrightarrow & \mathbb{R} \\
    & \mu & \longmapsto &  \frac{1}{n} \sum_{i=1}^{n} |y_{i}-\mu|
\end{array}$$
The function f is a convex function. So there will be an optimal solution.\\
Our optimization function is not always smooth, this implies the non-existence of the gradient in the critical points.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Quantiles}
\label{sub:Quantiles}
\begin{frame}{Quantile}
  \framesubtitle{Sous-sections masquées}
\mytheorem{Quantiles Definition}
{
Let $Y$ be a real valued random variable with cumulative distribution function $ F_{Y}(y)=P(Y\leq y)$.
The $\alpha$-th quantile of $Y$ is given by :
$$
 Q_{Y}(\alpha )=F_{Y}^{-1}(\alpha )=\inf \left\{y:F_{Y}(y)\geq \alpha \right\}
$$
where  $\alpha \in ]0,1[$
}

\vspace{0.25cm}

\underline{\textbf{Remark :}}\\
let define the loss function $l_{\alpha}$ : 
$$\begin{array}{l|rcl}
    l_{\alpha} : & \mathbb{R} & \longrightarrow & \mathbb{R} \\
    & x & \longmapsto &  
    \left \{
   \begin{array}{r c l}
      -(1-\alpha) x \quad \text{si $x\leqslant 0$}\\
      \alpha x \quad \text{si $x\geqslant 0$}
   \end{array}
   \right .
\end{array}$$

$$\quad  l_{\alpha}:x \rightarrow \alpha |x|~  \mathds{1}_{x >0} + (1-\alpha) |x|~ \mathds{1}_{x \leqslant 0}$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Quantiles}
A specific quantile can be found by minimizing the expected loss of $Y-\mu$ with respect to $\mu$  \\

$$
\underset {\mu \in \mathbb{R}}{\min }E(l _{\alpha }(Y-\mu))= {\underset {\mu \in \mathbb{R}}{\min }} \left\{(\alpha -1)\int _{-\infty }^{\mu}(y-\mu)dF_{Y}(y)+\alpha \int _{\mu}^{\infty }(y-\mu)dF_{Y}(y)\right\}.
$$

This can be shown by setting the derivative of the expected loss function to $0$ and letting $q_{\alpha }$ be the solution of

$$
0=(1-\alpha)\int_{-\infty}^{q_{\alpha}}dF_{Y}(y)-\alpha\int_{q_{\alpha}}^{\infty}dF_{Y}(y).
$$
This equation reduces to
$$
0=F_{Y}(q_{\alpha })-\alpha
$$
and then to
$$
F_{Y}(q_{\alpha })=\alpha .
$$
Hence $ q_{\alpha}$is $\alpha$ th quantile of the random variable Y.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quantile regression}
\label{sec:Quantile regression}

\begin{frame}{Quantile regression}
$y_{1},\dots, y_{n}\in \mathbb{R}$ observations, $x_{1},\dots, x_{n}\in \mathbb{R}^p$ explanatory variables \\
\begin{itemize}
    \item The quantile regression is described by the following equation:
    $$y_i = x_i^T \beta^{\alpha} + \epsilon_i$$
    where $\beta^{\alpha}$ is the vector of unknown parameters associated with the $q^{th}$ quantile
    \item The OLS minimizes $\sum_i \epsilon_i^2$, the sum of squares of the model prediction error $\epsilon_i$
    \item The median regression, also called least absolute-deviation regression minimizes $\sum_i \left | \epsilon_i \right |$
    \item The quantile regression minimizes 
    $$ E(l _{\alpha }(Y-X\beta )) = \sum_i \alpha \left | \epsilon_i \right | + \sum_i (1 - \alpha) \left | \epsilon_i \right |$$
    a sum that gives the asymmetric penalties $  \alpha \left | \epsilon_i \right |$ underprediction and $(1 - \alpha) \left | \epsilon_i \right |$ overprediction
   
\end{itemize}   
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Quantile regression}


\underline{\textbf{Remark :}}
$$ \epsilon_i = y_i - x_i^T \beta^{\alpha}$$

\mytheorem{Definition}
{
Let $\alpha \in ]0,1[$. The $\alpha$th quantile regression estimator $\hat{\beta}^{\alpha}$ of $\beta$
$$\hat{\beta}^{\alpha}\in \operatorname*{argmin}_{\beta\in \mathds{R}^p}\dfrac{1}{n}\sum_{i=1}^{n}l_{\alpha}(y_{i}-x_{i}^T \beta)$$
$$X= \begin{bmatrix} x_{1}^T \\ .\\.\\.\\
x_{n}^T
\end{bmatrix} \in \mathds{R}^{n*p}$$
}

\vspace{0.25cm}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Advantages}
\label{sec:Advantages}
\begin{frame}{Advantages}

\begin{itemize}
    \item Flexibility for modeling data with heterogeneous conditional distributions.
    \item Median regression is more robust to outliers than the OLS regression.
    \item Richer characterization and description of the data: can show different effects of the independent variables on the dependent variable depending across the spectrum of the dependent variable.
\end{itemize}   

\end{frame}

\end{document}
